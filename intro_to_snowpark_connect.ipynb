{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a715f2c3",
   "metadata": {},
   "source": [
    "# Introduction to Snowpark Connect for Apache Spark\n",
    "\n",
    "### What You'll Learn:\n",
    "- How Snowpark Connect executes PySpark on Snowflake infrastructure\n",
    "- Data ingestion patterns (tables, stages, cloud storage)\n",
    "- Transformations, joins, and aggregations\n",
    "- Writing data with partitioning and compression\n",
    "- Building production pipelines with telemetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6495df",
   "metadata": {},
   "source": [
    "## What is Snowpark Connect?\n",
    "\n",
    "Snowpark Connect allows you to run the **PySpark DataFrame API** on **Snowflake infrastructure**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e10a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from snowflake import snowpark_connect\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "# initialize session\n",
    "session = get_active_session()\n",
    "spark = snowpark_connect.server.init_spark_session()\n",
    "\n",
    "# print session info\n",
    "print(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f059866",
   "metadata": {},
   "source": [
    "### Key Concepts:\n",
    "\n",
    "**Execution Model:**\n",
    "- Your DataFrame operations are translated to Snowflake SQL\n",
    "- Computation happens in Snowflake warehouses\n",
    "- Results stream back via Apache Arrow format\n",
    "- No Spark cluster, driver, or executors\n",
    "\n",
    "**Query Pushdown:**\n",
    "- âœ… **Fully Optimized:** DataFrame operations, SQL functions, aggregations push down to Snowflake\n",
    "- âš ï¸ **Performance Impact:** Python UDFs run client-side (fetch data â†’ process â†’ send back)\n",
    "- ðŸ’¡ **Better Alternative:** Use built-in SQL functions instead of UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4348f46",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA INGESTION METHODS\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, BooleanType\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. READ FROM SNOWFLAKE TABLES\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Read table by name\n",
    "#   - Fastest method - data is already in Snowflake, no file parsing\n",
    "df = spark.read.table(\"MY_DATABASE.MY_SCHEMA.MY_TABLE\")\n",
    "\n",
    "# Execute SQL query and return results\n",
    "#   - Useful for filtering at read time to reduce data transfer\n",
    "#   - Supports all Snowflake SQL syntax\n",
    "df = spark.sql(\"SELECT * FROM MY_TABLE WHERE status = 'active' LIMIT 1000\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. READ FROM SNOWFLAKE STAGES\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# PARQUET:\n",
    "df = spark.read.parquet(\"@MY_STAGE/path/to/data.parquet\")\n",
    "\n",
    "# CSV:\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", False) \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"quote\", '\"') \\\n",
    "    .option(\"escape\", \"\\\\\") \\\n",
    "    .option(\"nullValue\", \"\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .load(\"@MY_STAGE/path/to/data.csv\")\n",
    "\n",
    "# JSON:\n",
    "df = spark.read.format(\"json\") \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .option(\"allowComments\", True) \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .load(\"@MY_STAGE/path/to/data.json\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. DIRECT CLOUD STORAGE ACCESS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Read from S3\n",
    "#   - Requires AWS credentials configured\n",
    "df = spark.read.parquet(\"s3://my-bucket/path/to/data/\")\n",
    "\n",
    "# Read from Google Cloud Storage\n",
    "#   - Requires GCP credentials configured\n",
    "df = spark.read.parquet(\"gs://my-bucket/path/to/data/\")\n",
    "\n",
    "# Read from Azure Blob Storage\n",
    "#   - Requires Azure credentials configured\n",
    "df = spark.read.parquet(\"wasbs://container@account.blob.core.windows.net/path/to/data/\")\n",
    "\n",
    "# NOTE: Using Snowflake stages is often simpler (handles auth automatically)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. READ MULTIPLE FILES\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Read with wildcard patterns\n",
    "#   - Match all files of a type in a directory\n",
    "df = spark.read.parquet(\"@MY_STAGE/data/*.parquet\")\n",
    "#   - Match files with a naming pattern\n",
    "df = spark.read.csv(\"@MY_STAGE/logs/2024-*/events_*.csv\")\n",
    "\n",
    "# Read recursively from nested directories\n",
    "#   - Searches all subdirectories\n",
    "df = spark.read.option(\"recursiveFileLookup\", True).parquet(\"@MY_STAGE/nested_data/\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# =============================================================================\n",
    "# HANDLING LARGE CSVs\n",
    "# ============================================================================\n",
    "\n",
    "# âŒ SLOW PATTERN:\n",
    "df_slow = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"@MY_STAGE/large_file.csv\")\n",
    "#    Problem: Scans entire file just to guess column types\n",
    "\n",
    "# âœ… FAST PATTERN:\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"created_at\", TimestampType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "df_fast = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"@MY_STAGE/large_file.csv\")\n",
    "#    Benefit: No extra scan\n",
    "\n",
    "# âœ… COMPRESSED FILES:\n",
    "df_gzip = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"@MY_STAGE/data.csv.gz\")\n",
    "#    Benefit: Smaller transfer size, auto-decompressed on read\n",
    "\n",
    "# âœ… BEST PRACTICE - Convert to Parquet:\n",
    "df_csv = spark.read.format(\"csv\").option(\"header\", True).schema(schema).load(\"@MY_STAGE/raw.csv\")\n",
    "df_csv.write.mode(\"overwrite\").parquet(\"@MY_STAGE/optimized/data.parquet\")\n",
    "df_optimized = spark.read.parquet(\"@MY_STAGE/optimized/data.parquet\")\n",
    "#    Benefit: Fast columnar reads for repeated analysis\n",
    "\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f74460b",
   "metadata": {},
   "source": [
    "## Writing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513bab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WRITING DATA\n",
    "# =============================================================================\n",
    "#\n",
    "# Write transformed data back to Snowflake tables, stages, or cloud storage\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. WRITE TO SNOWFLAKE TABLES\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Save DataFrame as a Snowflake table\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"MY_TABLE\")\n",
    "df.write.mode(\"append\").saveAsTable(\"MY_TABLE\")\n",
    "df.write.mode(\"ignore\").saveAsTable(\"MY_TABLE\")\n",
    "df.write.mode(\"error\").saveAsTable(\"MY_TABLE\")  # default behavior\n",
    "\n",
    "# Write to fully qualified table name\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"MY_DATABASE.MY_SCHEMA.MY_TABLE\")\n",
    "\n",
    "# NOTE: This is the fastest write path - data stays in Snowflake\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. WRITE TO SNOWFLAKE STAGES\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# PARQUET (recommended):\n",
    "df.write.mode(\"overwrite\").parquet(\"@MY_STAGE/output/data.parquet\")\n",
    "#   - Best for analytical workloads\n",
    "#   - Columnar format, compressed by default\n",
    "#   - Preserves schema information\n",
    "\n",
    "# CSV:\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"quote\", '\"') \\\n",
    "    .csv(\"@MY_STAGE/output/data.csv\")\n",
    "\n",
    "# JSON:\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .json(\"@MY_STAGE/output/data.json\")\n",
    "#   - One JSON object per row\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. WRITE TO CLOUD STORAGE\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Write to S3\n",
    "df.write.mode(\"overwrite\").parquet(\"s3://my-bucket/output/data/\")\n",
    "\n",
    "# Write to Google Cloud Storage\n",
    "df.write.mode(\"overwrite\").parquet(\"gs://my-bucket/output/data/\")\n",
    "\n",
    "# Write to Azure Blob Storage\n",
    "df.write.mode(\"overwrite\").parquet(\"wasbs://container@account.blob.core.windows.net/output/data/\")\n",
    "\n",
    "# NOTE: Using Snowflake stages is often simpler (handles auth automatically)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. PARTITIONING FOR PERFORMANCE\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Write with partitioning by columns\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"@MY_STAGE/partitioned_data/\")\n",
    "\n",
    "# Single column partition (common for date-based)\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"event_date\") \\\n",
    "    .parquet(\"@MY_STAGE/daily_data/\")\n",
    "\n",
    "# Benefits:\n",
    "#   - Faster queries that filter on partition columns\n",
    "#   - Enables parallel reads of different partitions\n",
    "#   - Easier data management (delete old partitions)\n",
    "\n",
    "# Considerations:\n",
    "#   - Too many partitions = too many small files\n",
    "#   - Choose columns with moderate cardinality\n",
    "#   - Date-based partitioning is very common\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. COMPRESSION OPTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Snappy (default for Parquet) - fast, moderate compression\n",
    "df.write.option(\"compression\", \"snappy\").parquet(\"@MY_STAGE/snappy_data/\")\n",
    "\n",
    "# Gzip - slower, better compression\n",
    "df.write.option(\"compression\", \"gzip\").parquet(\"@MY_STAGE/gzip_data/\")\n",
    "\n",
    "# LZ4 - very fast, moderate compression\n",
    "df.write.option(\"compression\", \"lz4\").parquet(\"@MY_STAGE/lz4_data/\")\n",
    "\n",
    "# Zstd - good balance of speed and compression\n",
    "df.write.option(\"compression\", \"zstd\").parquet(\"@MY_STAGE/zstd_data/\")\n",
    "\n",
    "# None - no compression (rarely recommended)\n",
    "df.write.option(\"compression\", \"none\").parquet(\"@MY_STAGE/uncompressed_data/\")\n",
    "\n",
    "# Trade-offs:\n",
    "#   - Higher compression = smaller files but slower writes\n",
    "#   - Snappy/LZ4 good for frequently accessed data\n",
    "#   - Gzip/Zstd good for archival or infrequent access\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. CONTROLLING OUTPUT FILE COUNT\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Reduce number of output files (coalesce)\n",
    "df.coalesce(1).write.mode(\"overwrite\").parquet(\"@MY_STAGE/single_file/\")   # Single file\n",
    "df.coalesce(4).write.mode(\"overwrite\").parquet(\"@MY_STAGE/few_files/\")     # 4 files max\n",
    "\n",
    "# Increase number of output files (repartition)\n",
    "df.repartition(100).write.mode(\"overwrite\").parquet(\"@MY_STAGE/many_files/\")\n",
    "\n",
    "# Repartition by column (ensures related data in same files)\n",
    "df.repartition(\"customer_id\").write.mode(\"overwrite\").parquet(\"@MY_STAGE/by_customer/\")\n",
    "\n",
    "# Best practices:\n",
    "#   - Target 100MB-1GB per file for optimal read performance\n",
    "#   - Too many small files = slow reads (file listing overhead)\n",
    "#   - Too few large files = limited parallelism\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. WRITE MODES SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Overwrite - Replaces all existing data\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"MY_TABLE\")\n",
    "#   - Use when you want a fresh snapshot\n",
    "#   - âš ï¸  Destructive - existing data is lost\n",
    "\n",
    "# Append - Adds new rows to existing data\n",
    "df.write.mode(\"append\").saveAsTable(\"MY_TABLE\")\n",
    "#   - Use for incremental loads\n",
    "#   - Beware of duplicates if re-running\n",
    "\n",
    "# Ignore - Skips write if destination exists\n",
    "df.write.mode(\"ignore\").saveAsTable(\"MY_TABLE\")\n",
    "#   - Use for idempotent operations\n",
    "#   - No error, but data not written\n",
    "\n",
    "# Error (default) - Fails if destination exists\n",
    "df.write.mode(\"error\").saveAsTable(\"MY_TABLE\")\n",
    "#   - Safest option to prevent accidental overwrites\n",
    "#   - Must explicitly choose overwrite/append\n",
    "\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c92af",
   "metadata": {},
   "source": [
    "## Data Transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740d10b",
   "metadata": {},
   "source": [
    "### Feature Support Matrix:\n",
    "\n",
    "Understanding what PySpark features are supported helps you write efficient code.\n",
    "\n",
    "#### âœ… Fully Supported DataFrame Operations:\n",
    "- `select`, `filter`, `where`\n",
    "- `groupBy`, `agg` (all aggregation functions)\n",
    "- `join` (inner, left, right, outer, broadcast)\n",
    "- `orderBy`, `sort`\n",
    "- `distinct`, `dropDuplicates`\n",
    "- Window functions (`row_number`, `rank`, `lag`, `lead`, etc.)\n",
    "- Built-in functions (95%+ coverage)\n",
    "- `cache`, `persist` (creates temp tables in Snowflake)\n",
    "\n",
    "#### âš ï¸ Limited Support:\n",
    "- `repartition` (logical operation only)\n",
    "- `coalesce` (similar to repartition)\n",
    "- Python UDFs (work but slow - avoid if possible)\n",
    "- Pandas UDFs (work but slow - avoid if possible)\n",
    "- MLlib (partial - transformers work, estimators limited)\n",
    "\n",
    "#### âŒ NOT Supported:\n",
    "- RDD API completely\n",
    "- `.rdd`, `.foreach()`, `.foreachPartition()`\n",
    "- Structured Streaming\n",
    "- GraphX\n",
    "- Custom data sources\n",
    "- `.checkpoint()`\n",
    "\n",
    "\n",
    "### Data Types Support\n",
    "\n",
    "**âœ… Supported:**\n",
    "- String, Integer, Long, Float, Double, Decimal\n",
    "- Boolean, Date, Timestamp\n",
    "- Array, Map, Struct\n",
    "- Binary\n",
    "\n",
    "**âŒ Not Supported:**\n",
    "- DayTimeIntervalType\n",
    "- YearMonthIntervalType\n",
    "- UserDefinedTypes\n",
    "\n",
    "#### Supported File Formats:\n",
    "- âœ… Parquet, CSV, JSON, Avro, ORC\n",
    "- âŒ Delta Lake, Hudi not supported\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda9f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA TRANSFORMATIONS\n",
    "# =============================================================================\n",
    "#\n",
    "# All transformations push down to Snowflake SQL - no data leaves the warehouse\n",
    "# until you explicitly collect results\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when, coalesce,\n",
    "    sum, avg, count, min, max, countDistinct, collect_list,\n",
    "    year, month, dayofmonth, hour, minute, second, dayofweek, dayofyear, weekofyear, quarter,\n",
    "    to_date, to_timestamp, date_add, date_sub, months_between, datediff, date_trunc,\n",
    "    upper, lower, initcap, trim, ltrim, rtrim, lpad, rpad,\n",
    "    concat, substring, regexp_replace, regexp_extract,\n",
    "    row_number, rank, dense_rank, percent_rank, lag, lead, first, last,\n",
    "    broadcast\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. SELECTING AND FILTERING\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Select specific columns from a DataFrame\n",
    "df_selected = df.select(\"id\", \"name\", \"amount\")\n",
    "df_selected = df.select(col(\"id\"), col(\"name\").alias(\"customer_name\"))  # with rename\n",
    "\n",
    "# Filter rows based on conditions\n",
    "df_filtered = df.filter(col(\"status\") == \"active\")\n",
    "df_filtered = df.where(col(\"amount\") > 100)\n",
    "df_filtered = df.filter((col(\"status\") == \"active\") & (col(\"amount\") > 100))  # AND\n",
    "df_filtered = df.filter((col(\"status\") == \"active\") | (col(\"amount\") > 1000)) # OR\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_unique = df.distinct()                                    # all columns\n",
    "df_unique = df.dropDuplicates([\"customer_id\", \"order_date\"]) # specific columns\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. ADDING AND MODIFYING COLUMNS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Add a new column with calculated values\n",
    "df = df.withColumn(\"total\", col(\"price\") * col(\"quantity\"))\n",
    "df = df.withColumn(\"discounted_price\", col(\"price\") * 0.9)\n",
    "df = df.withColumn(\"status_flag\", when(col(\"status\") == \"active\", 1).otherwise(0))\n",
    "\n",
    "# Rename existing columns\n",
    "df = df.withColumnRenamed(\"old_name\", \"new_name\")\n",
    "df = df.toDF(\"col1\", \"col2\", \"col3\")  # rename all columns at once\n",
    "\n",
    "# Cast column to different data type\n",
    "df = df.withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
    "df = df.withColumn(\"event_date\", col(\"timestamp_col\").cast(\"date\"))\n",
    "\n",
    "# Replace null values with defaults\n",
    "df = df.fillna(0, subset=[\"amount\"])                    # single column\n",
    "df = df.fillna({\"amount\": 0, \"name\": \"Unknown\"})        # multiple columns\n",
    "df = df.withColumn(\"value\", coalesce(col(\"value\"), lit(0)))  # using coalesce\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. AGGREGATIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Group rows by one or more columns\n",
    "df_grouped = df.groupBy(\"category\")\n",
    "df_grouped = df.groupBy(\"category\", \"region\")\n",
    "\n",
    "# Apply aggregate functions to groups\n",
    "df_agg = df.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"row_count\"),\n",
    "    countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "    sum(\"amount\").alias(\"total_amount\"),\n",
    "    avg(\"amount\").alias(\"avg_amount\"),\n",
    "    min(\"amount\").alias(\"min_amount\"),\n",
    "    max(\"amount\").alias(\"max_amount\"),\n",
    "    collect_list(\"product_name\").alias(\"products\")\n",
    ")\n",
    "\n",
    "# Rename aggregated columns with alias\n",
    "df_agg = df.groupBy(\"category\").agg(\n",
    "    sum(\"amount\").alias(\"total_sales\"),\n",
    "    avg(\"amount\").alias(\"average_sale\")\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. JOINS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Join two DataFrames on matching columns\n",
    "df_joined = df1.join(df2, df1[\"id\"] == df2[\"id\"], \"inner\")   # Inner join\n",
    "df_joined = df1.join(df2, \"id\", \"inner\")                      # Simpler syntax when column names match\n",
    "df_joined = df1.join(df2, \"customer_id\", \"left\")              # Left join\n",
    "df_joined = df1.join(df2, \"customer_id\", \"right\")             # Right join\n",
    "df_joined = df1.join(df2, \"customer_id\", \"outer\")             # Full outer join\n",
    "df_joined = df1.crossJoin(df2)                                # Cross join (cartesian)\n",
    "\n",
    "# Broadcast small tables for faster joins\n",
    "df_joined = df_large.join(broadcast(df_small), \"key_column\", \"left\")\n",
    "\n",
    "# Handle column name conflicts after join\n",
    "df_joined = df1.alias(\"a\").join(df2.alias(\"b\"), col(\"a.id\") == col(\"b.id\"))\n",
    "df_result = df_joined.select(col(\"a.id\"), col(\"a.name\"), col(\"b.value\"))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. WINDOW FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Define a window specification\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "window_spec_frame = Window.partitionBy(\"customer_id\").orderBy(\"order_date\").rowsBetween(-2, 0)\n",
    "\n",
    "# Ranking functions\n",
    "df = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "df = df.withColumn(\"rank\", rank().over(window_spec))              # gaps for ties\n",
    "df = df.withColumn(\"dense_rank\", dense_rank().over(window_spec))  # no gaps\n",
    "df = df.withColumn(\"pct_rank\", percent_rank().over(window_spec))\n",
    "\n",
    "# Analytic functions\n",
    "df = df.withColumn(\"prev_amount\", lag(\"amount\", 1).over(window_spec))\n",
    "df = df.withColumn(\"next_amount\", lead(\"amount\", 1).over(window_spec))\n",
    "df = df.withColumn(\"first_order\", first(\"amount\").over(window_spec))\n",
    "df = df.withColumn(\"last_order\", last(\"amount\").over(window_spec))\n",
    "\n",
    "# Running calculations\n",
    "window_running = Window.partitionBy(\"customer_id\").orderBy(\"order_date\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "df = df.withColumn(\"running_total\", sum(\"amount\").over(window_running))\n",
    "df = df.withColumn(\"running_avg\", avg(\"amount\").over(window_running))\n",
    "df = df.withColumn(\"running_min\", min(\"amount\").over(window_running))\n",
    "df = df.withColumn(\"running_max\", max(\"amount\").over(window_running))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. DATE AND TIME OPERATIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Extract components from timestamps\n",
    "df = df.withColumn(\"year\", year(\"timestamp_col\"))\n",
    "df = df.withColumn(\"month\", month(\"timestamp_col\"))\n",
    "df = df.withColumn(\"day\", dayofmonth(\"timestamp_col\"))\n",
    "df = df.withColumn(\"hour\", hour(\"timestamp_col\"))\n",
    "df = df.withColumn(\"minute\", minute(\"timestamp_col\"))\n",
    "df = df.withColumn(\"second\", second(\"timestamp_col\"))\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(\"timestamp_col\"))\n",
    "df = df.withColumn(\"day_of_year\", dayofyear(\"timestamp_col\"))\n",
    "df = df.withColumn(\"week\", weekofyear(\"timestamp_col\"))\n",
    "df = df.withColumn(\"quarter\", quarter(\"timestamp_col\"))\n",
    "\n",
    "# Convert strings to dates/timestamps\n",
    "df = df.withColumn(\"date_col\", to_date(\"date_string\", \"yyyy-MM-dd\"))\n",
    "df = df.withColumn(\"ts_col\", to_timestamp(\"ts_string\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Date arithmetic\n",
    "df = df.withColumn(\"next_week\", date_add(\"date_col\", 7))\n",
    "df = df.withColumn(\"last_week\", date_sub(\"date_col\", 7))\n",
    "df = df.withColumn(\"days_diff\", datediff(\"end_date\", \"start_date\"))\n",
    "df = df.withColumn(\"months_diff\", months_between(\"end_date\", \"start_date\"))\n",
    "df = df.withColumn(\"month_start\", date_trunc(\"month\", \"date_col\"))\n",
    "df = df.withColumn(\"week_start\", date_trunc(\"week\", \"date_col\"))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. STRING OPERATIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Transform string case\n",
    "df = df.withColumn(\"upper_name\", upper(\"name\"))\n",
    "df = df.withColumn(\"lower_name\", lower(\"name\"))\n",
    "df = df.withColumn(\"title_name\", initcap(\"name\"))\n",
    "\n",
    "# Pattern matching and replacement\n",
    "df = df.withColumn(\"has_email\", col(\"text\").contains(\"@\"))\n",
    "df = df.withColumn(\"cleaned\", regexp_replace(\"text\", \"[^a-zA-Z0-9]\", \"\"))\n",
    "df = df.withColumn(\"domain\", regexp_extract(\"email\", \"@(.+)$\", 1))\n",
    "\n",
    "# Trim and pad strings\n",
    "df = df.withColumn(\"trimmed\", trim(\"text\"))\n",
    "df = df.withColumn(\"left_trimmed\", ltrim(\"text\"))\n",
    "df = df.withColumn(\"right_trimmed\", rtrim(\"text\"))\n",
    "df = df.withColumn(\"padded_id\", lpad(\"id\", 10, \"0\"))    # \"42\" -> \"0000000042\"\n",
    "df = df.withColumn(\"padded_right\", rpad(\"code\", 5, \"X\")) # \"AB\" -> \"ABXXX\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8. SORTING AND LIMITING\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Sort results by one or more columns\n",
    "df_sorted = df.orderBy(\"name\")                              # ascending (default)\n",
    "df_sorted = df.orderBy(col(\"amount\").desc())                # descending\n",
    "df_sorted = df.orderBy(col(\"category\").asc(), col(\"amount\").desc())  # multiple columns\n",
    "df_sorted = df.orderBy(col(\"value\").asc_nulls_first())      # nulls first\n",
    "df_sorted = df.orderBy(col(\"value\").desc_nulls_last())      # nulls last\n",
    "\n",
    "# Limit number of rows returned\n",
    "df_limited = df.limit(10)                # first 10 rows\n",
    "df_top_10 = df.orderBy(col(\"amount\").desc()).limit(10)  # top 10 by amount\n",
    "\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce8eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# END-TO-END DATA PIPELINE\n",
    "# =============================================================================\n",
    "#\n",
    "# A production-ready pipeline structure with monitoring, error handling,\n",
    "# and best practices integrated throughout\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PIPELINE STRUCTURE OVERVIEW\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "# 1. Setup & Configuration\n",
    "# 2. Telemetry Initialization\n",
    "# 3. Data Ingestion \n",
    "# 4. Data Validation\n",
    "# 5. Transformations \n",
    "# 6. Data Quality Checks\n",
    "# 7. Write Output \n",
    "# 8. Cleanup & Summary\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: SETUP & CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# --- Import required packages ---\n",
    "import os\n",
    "import uuid\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "from snowflake import snowpark_connect\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, \n",
    "    StringType, IntegerType, LongType, DoubleType, \n",
    "    TimestampType, DateType, BooleanType\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when, coalesce, trim, lower, upper,\n",
    "    sum, avg, count, min, max, countDistinct,\n",
    "    year, month, dayofmonth, to_date, current_timestamp,\n",
    "    row_number, broadcast\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# --- Configuration dataclass for type safety and clarity ---\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Centralized configuration for the data pipeline.\"\"\"\n",
    "    \n",
    "    # Pipeline metadata\n",
    "    pipeline_name: str = \"sales_analytics_pipeline\"\n",
    "    pipeline_version: str = \"1.0.0\"\n",
    "    environment: str = os.getenv(\"ENVIRONMENT\", \"dev\")\n",
    "    \n",
    "    # Source configuration\n",
    "    source_table: str = \"RAW_DATA.SALES.TRANSACTIONS\"\n",
    "    dimension_table: str = \"RAW_DATA.SALES.CUSTOMERS\"\n",
    "    \n",
    "    # Output configuration\n",
    "    output_table: str = \"ANALYTICS.SALES.DAILY_SUMMARY\"\n",
    "    output_mode: str = \"overwrite\"\n",
    "    \n",
    "    # Processing parameters\n",
    "    lookback_days: int = 7\n",
    "    min_expected_rows: int = 1000\n",
    "    max_expected_rows: int = 10_000_000\n",
    "    \n",
    "    # Quality thresholds\n",
    "    max_null_percentage: float = 0.05  # 5% max nulls allowed\n",
    "    row_count_variance_threshold: float = 0.20  # 20% variance allowed\n",
    "\n",
    "config = PipelineConfig()\n",
    "\n",
    "# --- Initialize Snowpark Connect session ---\n",
    "session = get_active_session()\n",
    "spark = snowpark_connect.server.init_spark_session()\n",
    "\n",
    "print(f\"Pipeline: {config.pipeline_name} v{config.pipeline_version}\")\n",
    "print(f\"Environment: {config.environment}\")\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: TELEMETRY INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# --- Configure logging ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(config.pipeline_name)\n",
    "\n",
    "# --- Pipeline run tracking ---\n",
    "@dataclass\n",
    "class PipelineMetrics:\n",
    "    \"\"\"Track metrics throughout pipeline execution.\"\"\"\n",
    "    run_id: str\n",
    "    start_time: datetime\n",
    "    end_time: Optional[datetime] = None\n",
    "    status: str = \"running\"\n",
    "    \n",
    "    # Stage metrics\n",
    "    rows_ingested: int = 0\n",
    "    rows_after_validation: int = 0\n",
    "    rows_after_transform: int = 0\n",
    "    rows_written: int = 0\n",
    "    \n",
    "    # Quality metrics\n",
    "    validation_passed: bool = False\n",
    "    quality_score: float = 0.0\n",
    "    warnings: List[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.warnings = self.warnings or []\n",
    "    \n",
    "    def add_warning(self, message: str):\n",
    "        self.warnings.append(message)\n",
    "        logger.warning(message)\n",
    "    \n",
    "    def duration_seconds(self) -> float:\n",
    "        end = self.end_time or datetime.now()\n",
    "        return (end - self.start_time).total_seconds()\n",
    "\n",
    "# Initialize metrics for this run\n",
    "metrics = PipelineMetrics(\n",
    "    run_id=str(uuid.uuid4())[:8],\n",
    "    start_time=datetime.now()\n",
    ")\n",
    "\n",
    "logger.info(f\"{'='*60}\")\n",
    "logger.info(f\"PIPELINE START | Run ID: {metrics.run_id}\")\n",
    "logger.info(f\"{'='*60}\")\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: DATA INGESTION\n",
    "# =============================================================================\n",
    "\n",
    "def ingest_data(spark: SparkSession, config: PipelineConfig) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Ingest source data with early filtering to minimize data movement.\n",
    "    \n",
    "    Best practices applied:\n",
    "    - Filter at read time to reduce data volume\n",
    "    - Select only required columns\n",
    "    - Use Snowflake tables for fastest path\n",
    "    \"\"\"\n",
    "    logger.info(f\"Ingesting data from: {config.source_table}\")\n",
    "    \n",
    "    # Calculate date range for filtering\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=config.lookback_days)\n",
    "    \n",
    "    # Read with predicate pushdown - filter happens in Snowflake\n",
    "    df = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            transaction_id,\n",
    "            customer_id,\n",
    "            product_id,\n",
    "            transaction_date,\n",
    "            quantity,\n",
    "            unit_price,\n",
    "            discount_pct,\n",
    "            region,\n",
    "            channel,\n",
    "            created_at\n",
    "        FROM {config.source_table}\n",
    "        WHERE transaction_date >= '{start_date.strftime('%Y-%m-%d')}'\n",
    "          AND transaction_date < '{end_date.strftime('%Y-%m-%d')}'\n",
    "    \"\"\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute ingestion\n",
    "logger.info(\"STEP 3: Data Ingestion\")\n",
    "df_raw = ingest_data(spark, config)\n",
    "metrics.rows_ingested = df_raw.count()\n",
    "logger.info(f\"Rows ingested: {metrics.rows_ingested:,}\")\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: DATA VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "class ValidationError(Exception):\n",
    "    \"\"\"Raised when critical validation fails.\"\"\"\n",
    "    pass\n",
    "\n",
    "def validate_dataframe(\n",
    "    df: DataFrame, \n",
    "    config: PipelineConfig,\n",
    "    metrics: PipelineMetrics\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Validate source data before processing.\n",
    "    \n",
    "    Checks performed:\n",
    "    - Row count within expected range\n",
    "    - Required columns present\n",
    "    - Null percentage within threshold\n",
    "    - No duplicate primary keys\n",
    "    \"\"\"\n",
    "    logger.info(\"STEP 4: Data Validation\")\n",
    "    errors = []\n",
    "    \n",
    "    # --- Check 1: Row count bounds ---\n",
    "    row_count = metrics.rows_ingested\n",
    "    if row_count < config.min_expected_rows:\n",
    "        errors.append(f\"Row count {row_count:,} below minimum {config.min_expected_rows:,}\")\n",
    "    if row_count > config.max_expected_rows:\n",
    "        errors.append(f\"Row count {row_count:,} exceeds maximum {config.max_expected_rows:,}\")\n",
    "    \n",
    "    # --- Check 2: Required columns present ---\n",
    "    required_columns = [\"transaction_id\", \"customer_id\", \"transaction_date\", \"quantity\", \"unit_price\"]\n",
    "    missing_columns = [c for c in required_columns if c not in df.columns]\n",
    "    if missing_columns:\n",
    "        errors.append(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # --- Check 3: Null percentage in key columns ---\n",
    "    key_columns = [\"transaction_id\", \"customer_id\", \"quantity\", \"unit_price\"]\n",
    "    for col_name in key_columns:\n",
    "        if col_name in df.columns:\n",
    "            null_count = df.filter(col(col_name).isNull()).count()\n",
    "            null_pct = null_count / row_count if row_count > 0 else 0\n",
    "            if null_pct > config.max_null_percentage:\n",
    "                errors.append(f\"Column '{col_name}' has {null_pct:.1%} nulls (max: {config.max_null_percentage:.1%})\")\n",
    "            elif null_pct > 0:\n",
    "                metrics.add_warning(f\"Column '{col_name}' has {null_pct:.1%} nulls\")\n",
    "    \n",
    "    # --- Check 4: No duplicate primary keys ---\n",
    "    pk_count = df.select(\"transaction_id\").distinct().count()\n",
    "    if pk_count < row_count:\n",
    "        duplicate_count = row_count - pk_count\n",
    "        errors.append(f\"Found {duplicate_count:,} duplicate transaction_ids\")\n",
    "    \n",
    "    # --- Handle validation results ---\n",
    "    if errors:\n",
    "        for error in errors:\n",
    "            logger.error(f\"VALIDATION FAILED: {error}\")\n",
    "        raise ValidationError(f\"Validation failed with {len(errors)} error(s)\")\n",
    "    \n",
    "    metrics.validation_passed = True\n",
    "    metrics.rows_after_validation = row_count\n",
    "    logger.info(\"Validation PASSED\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute validation\n",
    "df_validated = validate_dataframe(df_raw, config, metrics)\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: TRANSFORMATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def transform_data(\n",
    "    df: DataFrame,\n",
    "    df_customers: DataFrame,\n",
    "    metrics: PipelineMetrics\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply business transformations to the validated data.\n",
    "    \n",
    "    Stages:\n",
    "    5A. Data Cleaning\n",
    "    5B. Business Logic  \n",
    "    5C. Joins & Enrichment\n",
    "    5D. Aggregations\n",
    "    \"\"\"\n",
    "    logger.info(\"STEP 5: Transformations\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STAGE 5A: Data Cleaning\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger.info(\"  5A: Data Cleaning\")\n",
    "    \n",
    "    df_clean = df \\\n",
    "        .withColumn(\"region\", trim(upper(col(\"region\")))) \\\n",
    "        .withColumn(\"channel\", trim(lower(col(\"channel\")))) \\\n",
    "        .withColumn(\"discount_pct\", coalesce(col(\"discount_pct\"), lit(0.0))) \\\n",
    "        .filter(col(\"quantity\") > 0) \\\n",
    "        .filter(col(\"unit_price\") > 0)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STAGE 5B: Business Logic\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger.info(\"  5B: Business Logic\")\n",
    "    \n",
    "    df_enriched = df_clean \\\n",
    "        .withColumn(\n",
    "            \"gross_amount\", \n",
    "            col(\"quantity\") * col(\"unit_price\")\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"discount_amount\",\n",
    "            col(\"gross_amount\") * col(\"discount_pct\")\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"net_amount\",\n",
    "            col(\"gross_amount\") - col(\"discount_amount\")\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"transaction_year\",\n",
    "            year(\"transaction_date\")\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"transaction_month\",\n",
    "            month(\"transaction_date\")\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"size_category\",\n",
    "            when(col(\"net_amount\") >= 1000, \"large\")\n",
    "            .when(col(\"net_amount\") >= 100, \"medium\")\n",
    "            .otherwise(\"small\")\n",
    "        )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STAGE 5C: Joins & Enrichment\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger.info(\"  5C: Joins & Enrichment\")\n",
    "    \n",
    "    # Broadcast small dimension table for faster join\n",
    "    df_with_customer = df_enriched.join(\n",
    "        broadcast(df_customers.select(\n",
    "            col(\"customer_id\"),\n",
    "            col(\"customer_segment\"),\n",
    "            col(\"customer_tier\"),\n",
    "            col(\"acquisition_date\")\n",
    "        )),\n",
    "        on=\"customer_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Log join quality\n",
    "    total_rows = df_enriched.count()\n",
    "    matched_rows = df_with_customer.filter(col(\"customer_segment\").isNotNull()).count()\n",
    "    match_rate = matched_rows / total_rows if total_rows > 0 else 0\n",
    "    logger.info(f\"  Customer join match rate: {match_rate:.1%}\")\n",
    "    \n",
    "    if match_rate < 0.95:\n",
    "        metrics.add_warning(f\"Customer join match rate below 95%: {match_rate:.1%}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STAGE 5D: Aggregations\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger.info(\"  5D: Aggregations\")\n",
    "    \n",
    "    df_aggregated = df_with_customer.groupBy(\n",
    "        \"transaction_date\",\n",
    "        \"region\",\n",
    "        \"channel\",\n",
    "        \"customer_segment\",\n",
    "        \"size_category\"\n",
    "    ).agg(\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        sum(\"quantity\").alias(\"total_quantity\"),\n",
    "        sum(\"gross_amount\").alias(\"total_gross\"),\n",
    "        sum(\"discount_amount\").alias(\"total_discount\"),\n",
    "        sum(\"net_amount\").alias(\"total_net\"),\n",
    "        avg(\"net_amount\").alias(\"avg_transaction_value\"),\n",
    "        min(\"net_amount\").alias(\"min_transaction_value\"),\n",
    "        max(\"net_amount\").alias(\"max_transaction_value\")\n",
    "    )\n",
    "    \n",
    "    # Add metadata columns\n",
    "    df_final = df_aggregated \\\n",
    "        .withColumn(\"pipeline_run_id\", lit(metrics.run_id)) \\\n",
    "        .withColumn(\"processed_at\", current_timestamp())\n",
    "    \n",
    "    metrics.rows_after_transform = df_final.count()\n",
    "    logger.info(f\"  Rows after aggregation: {metrics.rows_after_transform:,}\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# Load dimension table\n",
    "df_customers = spark.read.table(config.dimension_table)\n",
    "\n",
    "# Execute transformations\n",
    "df_transformed = transform_data(df_validated, df_customers, metrics)\n",
    "\n",
    "# Cache if we need to access multiple times (quality checks + write)\n",
    "df_transformed.cache()\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: DATA QUALITY CHECKS\n",
    "# =============================================================================\n",
    "\n",
    "def quality_checks(\n",
    "    df: DataFrame,\n",
    "    metrics: PipelineMetrics\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Validate output data quality before writing.\n",
    "    \n",
    "    Returns quality score (0.0 - 1.0).\n",
    "    \"\"\"\n",
    "    logger.info(\"STEP 6: Data Quality Checks\")\n",
    "    checks_passed = 0\n",
    "    total_checks = 4\n",
    "    \n",
    "    row_count = metrics.rows_after_transform\n",
    "    \n",
    "    # --- Check 1: Non-zero output ---\n",
    "    if row_count > 0:\n",
    "        checks_passed += 1\n",
    "        logger.info(\"  âœ“ Output has rows\")\n",
    "    else:\n",
    "        logger.error(\"  âœ— Output is empty!\")\n",
    "    \n",
    "    # --- Check 2: No duplicate keys ---\n",
    "    key_columns = [\"transaction_date\", \"region\", \"channel\", \"customer_segment\", \"size_category\"]\n",
    "    distinct_keys = df.select(key_columns).distinct().count()\n",
    "    if distinct_keys == row_count:\n",
    "        checks_passed += 1\n",
    "        logger.info(\"  âœ“ No duplicate keys\")\n",
    "    else:\n",
    "        logger.error(f\"  âœ— Found {row_count - distinct_keys:,} duplicate keys\")\n",
    "    \n",
    "    # --- Check 3: Amounts are positive ---\n",
    "    negative_amounts = df.filter(col(\"total_net\") < 0).count()\n",
    "    if negative_amounts == 0:\n",
    "        checks_passed += 1\n",
    "        logger.info(\"  âœ“ All amounts positive\")\n",
    "    else:\n",
    "        metrics.add_warning(f\"Found {negative_amounts:,} rows with negative amounts\")\n",
    "    \n",
    "    # --- Check 4: Transaction counts make sense ---\n",
    "    total_transactions = df.agg(sum(\"transaction_count\")).collect()[0][0]\n",
    "    if total_transactions >= metrics.rows_after_validation * 0.9:\n",
    "        checks_passed += 1\n",
    "        logger.info(\"  âœ“ Transaction counts reconcile\")\n",
    "    else:\n",
    "        metrics.add_warning(\"Transaction count reconciliation variance > 10%\")\n",
    "    \n",
    "    quality_score = checks_passed / total_checks\n",
    "    logger.info(f\"  Quality Score: {quality_score:.0%} ({checks_passed}/{total_checks} checks passed)\")\n",
    "    \n",
    "    return quality_score\n",
    "\n",
    "# Execute quality checks\n",
    "metrics.quality_score = quality_checks(df_transformed, metrics)\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 7: WRITE OUTPUT\n",
    "# =============================================================================\n",
    "\n",
    "def write_output(\n",
    "    df: DataFrame,\n",
    "    config: PipelineConfig,\n",
    "    metrics: PipelineMetrics\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Write transformed data to destination table.\n",
    "    \n",
    "    Best practices:\n",
    "    - Use Snowflake tables for fastest write path\n",
    "    - Verify row count after write\n",
    "    \"\"\"\n",
    "    logger.info(\"STEP 7: Write Output\")\n",
    "    logger.info(f\"  Destination: {config.output_table}\")\n",
    "    logger.info(f\"  Mode: {config.output_mode}\")\n",
    "    \n",
    "    # Write to Snowflake table\n",
    "    df.write \\\n",
    "        .mode(config.output_mode) \\\n",
    "        .saveAsTable(config.output_table)\n",
    "    \n",
    "    # Verify write success\n",
    "    written_count = spark.read.table(config.output_table).count()\n",
    "    logger.info(f\"  Rows written: {written_count:,}\")\n",
    "    \n",
    "    return written_count\n",
    "\n",
    "# Execute write\n",
    "metrics.rows_written = write_output(df_transformed, config, metrics)\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 8: CLEANUP & SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "def cleanup_and_summarize(\n",
    "    df_cached: DataFrame,\n",
    "    metrics: PipelineMetrics\n",
    "):\n",
    "    \"\"\"\n",
    "    Release resources and log final summary.\n",
    "    \n",
    "    Always runs, even on failure (call in finally block).\n",
    "    \"\"\"\n",
    "    logger.info(\"STEP 8: Cleanup & Summary\")\n",
    "    \n",
    "    # --- Release cached DataFrames ---\n",
    "    try:\n",
    "        df_cached.unpersist()\n",
    "        logger.info(\"  Cached DataFrames released\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"  Could not unpersist DataFrame: {e}\")\n",
    "    \n",
    "    # --- Finalize metrics ---\n",
    "    metrics.end_time = datetime.now()\n",
    "    metrics.status = \"success\" if metrics.quality_score >= 0.75 else \"completed_with_warnings\"\n",
    "    \n",
    "    # --- Log summary ---\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    logger.info(\"PIPELINE SUMMARY\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    logger.info(f\"  Run ID:           {metrics.run_id}\")\n",
    "    logger.info(f\"  Status:           {metrics.status.upper()}\")\n",
    "    logger.info(f\"  Duration:         {metrics.duration_seconds():.1f} seconds\")\n",
    "    logger.info(f\"  Rows In:          {metrics.rows_ingested:,}\")\n",
    "    logger.info(f\"  Rows Out:         {metrics.rows_written:,}\")\n",
    "    logger.info(f\"  Quality Score:    {metrics.quality_score:.0%}\")\n",
    "    \n",
    "    if metrics.warnings:\n",
    "        logger.info(f\"  Warnings ({len(metrics.warnings)}):\")\n",
    "        for warning in metrics.warnings:\n",
    "            logger.info(f\"    - {warning}\")\n",
    "    \n",
    "    logger.info(f\"{'='*60}\")\n",
    "\n",
    "# Execute cleanup (in production, wrap main pipeline in try/finally)\n",
    "cleanup_and_summarize(df_transformed, metrics)\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# BEST PRACTICES CHECKLIST (Applied in this pipeline)\n",
    "# =============================================================================\n",
    "#\n",
    "# âœ… PERFORMANCE:\n",
    "#    - Filter at read time (predicate pushdown)\n",
    "#    - Broadcast small dimension tables in joins\n",
    "#    - Cache DataFrames accessed multiple times\n",
    "#    - Use built-in SQL functions (no Python UDFs)\n",
    "#\n",
    "# âœ… RELIABILITY:\n",
    "#    - Validate data at input (row counts, nulls, duplicates)\n",
    "#    - Validate data at output (quality checks)\n",
    "#    - Use dataclasses for type-safe configuration\n",
    "#    - Clean up resources (unpersist cached data)\n",
    "#\n",
    "# âœ… OBSERVABILITY:\n",
    "#    - Structured logging with timestamps\n",
    "#    - Unique run ID for tracing\n",
    "#    - Row counts logged at each stage\n",
    "#    - Quality score and warnings tracked\n",
    "#\n",
    "# âœ… MAINTAINABILITY:\n",
    "#    - Configuration separated from logic\n",
    "#    - Clear stage separation with comments\n",
    "#    - Functions with docstrings\n",
    "#    - Consistent naming conventions\n",
    "#\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4388f47",
   "metadata": {},
   "source": [
    "### Performance Considerations\n",
    "\n",
    "Follow these best practices to get optimal performance from Snowpark Connect.\n",
    "\n",
    "1. Use SQL Functions Over UDFs: Python UDFs require data to be transferred to the client, processed, and sent back - this is 10-100x slower than native operations!\n",
    "2. Broadcast Joins for Small Tables: When joining a large table with a small dimension table, use `broadcast()` to optimize the join.\n",
    "3. Cache Frequently Accessed DataFrames: Caching creates temporary tables in Snowflake for faster repeated access. Remember to `unpersist()` when done!\n",
    "4. Minimize Data Movement: Process data in Snowflake and only transfer final results. Avoid `collect()` on large datasets!\n",
    "5. Partition Awareness: Filter on partitioned columns to enable partition pruning and reduce data scanned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6653b09",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "**Official Documentation:**\n",
    "- [Snowflake Documentation](https://docs.snowflake.com/)\n",
    "- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
